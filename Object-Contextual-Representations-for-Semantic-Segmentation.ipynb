{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style = \"color:rgb(50,120,229)\">Object-Contextual Representations for Semantic Segmentation</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style = \"color:rgb(50,120,229)\">Paper Details</font>\n",
    "\n",
    "1. **Authors**: Yuhui Yuan, Xilin Chen, Jingdong Wang\n",
    "2. **Paper Link**: https://arxiv.org/pdf/1909.11065v2.pdf\n",
    "3. **Category**: Semantic Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style = \"color:rgb(50,120,229)\">Introduction</font>\n",
    "\n",
    "**Semantic Segmentation**: Assigning a label to each pixel in an image.\n",
    "\n",
    "**Approach**: Contextual Aggregation.\n",
    "\n",
    "**Motivation**: Class label assigned to one pixel is the category of the object that the pixel belongs to.\n",
    "\n",
    "### <font style = \"color:rgb(8,133,37)\">What does Context mean?</font>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "The context of one position typically refers to a set of positions, e.g., the surrounding pixels.\n",
    "</div>\n",
    "\n",
    "If we refer to another paper (**Context Based Object Categorization: A Critical Survey**), Contextual Features are used to represent the interaction of an object with its surroundings. It can be divided into the following 3 categories:\n",
    "1. **Semantic Context** - This focuses on object co-occurence and allows to correct label of one object without affecting the label of other objects. For example, a tree is more likely to co-occur with a plant than a whale.\n",
    "2. **Spatial Context** - This focuses on the position of objects. For example, a dog is more likely to be present above grass and below sky rater than above sky and below grass. \n",
    "3. **Scale Context** - This focuses on relative size of objects. For example, a car is relatively smaller than a truck and not the other way round.\n",
    "\n",
    "### <font style = \"color:rgb(8,133,37)\">Approach</font>\n",
    "The approach discussed in the paper consists of the following 3 steps:\n",
    "\n",
    "1. **Coarse Soft Segmentation** - This involves dividing the contextual pixels (surrounding pixels) into soft object regions. The word \"soft\" here means that our focus is NOT on carrying out accurate segmentation.\n",
    "2. **Object Region Representation** - We use the soft segmentation obtained from the above step and the pixel representation to represent each object region.\n",
    "3. **Object-Contextual Representation** (OCR) - We use the output from the above 2 steps along with Pixel-Region relation to obtain the augmented representations.\n",
    "\n",
    "<img src=\"images/paper1/image01.png\" alt=\"Pipeline of the approach\" title=\"The Pipeline of the approach\" />\n",
    "<center><b>Figure 1</b>: The pipeline of the approach discussed in the paper. <a href=\"https://arxiv.org/pdf/1909.11065v2.pdf\">Source</a></center>\n",
    "\n",
    "### <font style = \"color:rgb(8,133,37)\">Differences</font>\n",
    "\n",
    "**OCR vs Multi-Scale Context**\n",
    "\n",
    "1. OCR differentiates contextual pixels which belong to the same class to the contextual pixels which belong to different class.\n",
    "2. Multi-Scale Context approach only differentiates pixels present at different positions.\n",
    "\n",
    "**OCR vs other Relational Context schemes**\n",
    "\n",
    "The approach discussed in the paper considers not only the object region representations but also the pixel and pixel-region relations, unlike other approaches.\n",
    "\n",
    "It should also be mentioned here that the current approach is also a relational context approach.\n",
    "\n",
    "**OCR vs Coarse-to-fine Segmentation**\n",
    "\n",
    "While \"Coarse-to-fine Segmentation\" is also followed in the current approach, the difference is the way the coarse segmentation is used. The OCR approach uses the coarse segmentation to generate a contextual representation, whereas the other approaches use it directly as an extra representation.\n",
    "\n",
    "**OCR vs Region-wise Segmentation**\n",
    "\n",
    "The region-wise segmentation first groups the pixels into **super pixels** which are then assigned a label. OCR on the other hand, uses the grouped regions to learn a better labelling for the pixels, instead of directly using them for segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style = \"color:rgb(50,120,229)\">Approach</font>\n",
    "\n",
    "It's now time to go into the mathematical details of the approach.\n",
    "\n",
    "### <font style = \"color:rgb(8,133,37)\">Semantic Segmentation - Problem Statement</font>\n",
    "\n",
    "Given $K$ classes, assign each pixel $p_i$ of image $I$ a label $l_i$ (which is one of the $K$ <b>unique</b> classes).\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Multi-Scale Context (Optional)</b>\n",
    "\n",
    "Multi-Scale context can be represented by the following equation:\n",
    "\n",
    "$$y_i ^d = \\sum_{p_s = p_i + d \\Delta_t} K_t ^ d x_s \\tag{1}$$\n",
    "\n",
    "Where,\n",
    "\n",
    "$y_i^d$ is the <b>output</b> representation of position $p_i$ for the $d$th dilated convolution,\n",
    "\n",
    "$d$ is the dilation rate,\n",
    "\n",
    "$t$ is the index of convolution (-1,0,1 for a 3x3 convolution),\n",
    "\n",
    "$\\Delta_t = (\\Delta_w, \\Delta_h) \\mid \\Delta_w = -1, 0, 1, \\Delta_h = -1,0,1$ for a 3x3 convolution,\n",
    "\n",
    "$x_s$ is the representation at $p_s$,\n",
    "\n",
    "$K^d$ is the kernel for $d$th dilated convolution\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Relational Context (Optional)</b>\n",
    "\n",
    "Relational context can be represented by the following equation:\n",
    "\n",
    "$$y_i = \\rho \\left(\\sum_{s \\in I} w_{is} \\delta(x_s) \\right) \\tag{2}$$\n",
    "\n",
    "Where,\n",
    "\n",
    "$y_i$ is the <b>output</b> representation of position $p_i$,\n",
    "\n",
    "$I$ refers to the image,\n",
    "\n",
    "$w_{is}$ is the relation between $x_i$ and $x_s$,\n",
    "\n",
    "$\\delta(\\cdot)$ and $\\rho(\\cdot)$ are transform functions,\n",
    "\n",
    "$x_s$ is the representation at $p_s$\n",
    "</div>\n",
    "\n",
    "Next comes my favorite part, the formulation of the current approach.\n",
    "\n",
    "### <font style = \"color:rgb(8,133,37)\">Step 1: Soft Object Regions</font>\n",
    "\n",
    "The image $I$ is partitioned into $K$ soft object regions: {$M_1, M_2, ..., M_K$} where $M_i$ corresponds to class $i$.\n",
    "\n",
    "$M_i$ is a 2D map where each entry represents the probability of the corresponding pixel belonging to the class $i$. \n",
    "\n",
    "### <font style = \"color:rgb(8,133,37)\">Step 2: Object Region Representation</font>\n",
    "\n",
    "The representations of all the pixels obtained in step 1 are aggregated as follows:\n",
    "\n",
    "$$f_k = \\sum_{i \\in I} m_{ki} x_{i} \\tag{3}$$\n",
    "\n",
    "Where, $m_{ki}$ represents the **normalized** probability of the pixel $p_i$ belonging to class $k$.\n",
    "\n",
    "### <font style = \"color:rgb(8,133,37)\">Step 3: Object Contextual Representation</font>\n",
    "\n",
    "First we obtain the relation between a pixel and an object region:\n",
    "\n",
    "$$w_{ik} = \\cfrac{e^{\\kappa(x_i, f_k)}}{\\sum_{j=1}^{K} e^{\\kappa(x_i, f_k)}}\\tag{4}$$\n",
    "\n",
    "Where, \n",
    "\n",
    "$\\kappa (\\cdot)$ is a relation function,\n",
    "\n",
    "Finally, we can obtain the object contextual representation $y_i$ for pixel $p_i$ as shown below:\n",
    "\n",
    "$$y_i = \\rho \\left(\\sum_{k=1}^{K} w_{ik} \\delta(f_k) \\right) \\tag{5}$$\n",
    "\n",
    "Notice the similarity between equation (5) and equation (2). \n",
    "\n",
    "### <font style = \"color:rgb(8,133,37)\">Step 4: Augmented Representation</font>\n",
    "\n",
    "The final representation for pixel $p_i$ is calculated as follows:\n",
    "\n",
    "$$z_i = g([x_i^T y_i^T]^T) \\tag{6}$$\n",
    "\n",
    "$g(\\cdot)$ here is a transform function with the only purpose to join the effect of original representation $x_i$ and object contextual representation $y_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style = \"color:rgb(50,120,229)\">References</font>\n",
    "1. Object-Contextual Representations for Semantic Segmentation - https://arxiv.org/pdf/1909.11065v2.pdf\n",
    "2. Context Based Object Categorization: A Critical Survey - https://vision.cornell.edu/se3/wp-content/uploads/2014/09/context_review08_0.pdf\n",
    "3. Jupyter Markdown - https://www.ibm.com/support/knowledgecenter/en/SSGNPV_1.1.3/dsx/markd-jupyter.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style = \"color:rgb(50,120,229)\">Network Architecture</font>\n",
    "\n",
    "Code Source: https://github.com/rosinality/ocr-pytorch (by **Kim Seonghyeon**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we already know that in PyTorch, everything is clubbed into a class and one of the most important functions we are looking for is **`forward`** for forward propagation implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(in_channel, out_channel, kernel_size):\n",
    "    layers = [\n",
    "        nn.Conv2d(\n",
    "            in_channel, out_channel, kernel_size, padding=kernel_size // 2, bias=False\n",
    "        ),\n",
    "        nn.BatchNorm2d(out_channel),\n",
    "        nn.ReLU(),\n",
    "    ]\n",
    "\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1d(in_channel, out_channel):\n",
    "    layers = [\n",
    "        nn.Conv1d(in_channel, out_channel, 1, bias=False),\n",
    "        nn.BatchNorm1d(out_channel),\n",
    "        nn.ReLU(),\n",
    "    ]\n",
    "\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OCR(nn.Module):\n",
    "    def __init__(self, n_class, backbone, feat_channels=[768, 1024]):\n",
    "        super().__init__()\n",
    "\n",
    "        self.backbone = backbone\n",
    "\n",
    "        ch16, ch32 = feat_channels\n",
    "\n",
    "        self.L = nn.Conv2d(ch16, n_class, 1)\n",
    "        self.X = conv2d(ch32, 512, 3)\n",
    "\n",
    "        self.phi = conv1d(512, 256)\n",
    "        self.psi = conv1d(512, 256)\n",
    "        self.delta = conv1d(512, 256)\n",
    "        self.rho = conv1d(256, 512)\n",
    "        self.g = conv2d(512 + 512, 512, 1)\n",
    "\n",
    "        self.out = nn.Conv2d(512, n_class, 1)\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "    def forward(self, input, target=None):\n",
    "        input_size = input.shape[2:]\n",
    "        stg16, stg32 = self.backbone(input)[-2:]\n",
    "\n",
    "        X = self.X(stg32)\n",
    "        L = self.L(stg16)\n",
    "        batch, n_class, height, width = L.shape\n",
    "        l_flat = L.view(batch, n_class, -1)\n",
    "        # M: NKL\n",
    "        M = torch.softmax(l_flat, -1)\n",
    "        channel = X.shape[1]\n",
    "        X_flat = X.view(batch, channel, -1)\n",
    "        # f_k: NCK\n",
    "        f_k = (M @ X_flat.transpose(1, 2)).transpose(1, 2)\n",
    "\n",
    "        # query: NKD\n",
    "        query = self.phi(f_k).transpose(1, 2)\n",
    "        # key: NDL\n",
    "        key = self.psi(X_flat)\n",
    "        logit = query @ key\n",
    "        # attn: NKL\n",
    "        attn = torch.softmax(logit, 1)\n",
    "\n",
    "        # delta: NDK\n",
    "        delta = self.delta(f_k)\n",
    "        # attn_sum: NDL\n",
    "        attn_sum = delta @ attn\n",
    "        # x_obj = NCHW\n",
    "        X_obj = self.rho(attn_sum).view(batch, -1, height, width)\n",
    "\n",
    "        concat = torch.cat([X, X_obj], 1)\n",
    "        X_bar = self.g(concat)\n",
    "        out = self.out(X_bar)\n",
    "        out = F.interpolate(out, size=input_size, mode='bilinear', align_corners=False)\n",
    "\n",
    "        if self.training:\n",
    "            aux_out = F.interpolate(\n",
    "                L, size=input_size, mode='bilinear', align_corners=False\n",
    "            )\n",
    "\n",
    "            loss = self.criterion(out, target)\n",
    "            aux_loss = self.criterion(aux_out, target)\n",
    "\n",
    "            return {'loss': loss, 'aux': aux_loss}, None\n",
    "\n",
    "        else:\n",
    "            return {}, out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that all the transform functions have been implemented as `1 x 1 conv --> BN --> ReLU` as per the original paper."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
